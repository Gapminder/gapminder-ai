custom_wrappers:
  model_config_wrapper:
    class: ./model_config_wrapper.ModelConfigWrapper
    config_cls: ./model_config_wrapper_config.ModelConfigWrapperConfig

custom_variation_generators:
  model_config_generator:
    class: ./model_config_variation_generator.ModelConfigVariationGenerator
    config_cls: ./model_config_variation_generator_config.ModelConfigVariationGeneratorConfig

custom_function: model_compare.model_compare

dataset:
  source_type: dataset
  file_path: "data/questions.csv"
  reader: csv_reader
  reader_config:
    expected_result_column: correct_answer

description: Example config

evaluators:
  # TODO: write custom evaluator to evaluate correct rate based on all records (evvaluator_type=all)
  - evaluator_type: individual
    metric_calculators:
      - method: AVERAGE   # Note: only average is supported for now
    name: openai_prompt_based_evaluator  # this evaluator will use gpt-4
    prompt: |-
      You are assessing a submitted answer on a given task based on a criterion. Here is the data:
      - Task: Given a multiple choice question, pick the correct answer.
      - Does the answer correct?
      [Input]: {question_text}
      A. {option_a}
      B. {option_b}
      C. {option_c}
      Correctness of Options:
      {option_a}: {option_a_correctness}
      {option_b}: {option_b_correctness}
      {option_c}: {option_c_correctness}
      [Result]: {raw_output}
      Answer the question by selecting one of the following options:
      A It doesn't mention what is the answer to the multiple choice question at all.
      B It mentions the answer to the multiple choice question, but it's not in the options list. (Not in option list)
      C It mentions the answer to the multiple choice question, but the answer is far away from the correct answer. (Very Wrong)
      D It mentions the answer to the multiple choice question, but the answer is not correct, though close to the correct one. (Wrong)
      E It mentions the answer to the multiple choice question, and the answer is correct. (Correct)
    display_name: correctness
    choices: ["A", "B", "C", "D", "E"]
    description: Does the answer correct?
    scale_description: "0-4"
    choice_scores:
      A: 0
      B: 1
      C: 2
      D: 3
      E: 4


variations:
  - name: model_config
    generator_name: model_config_generator
    generator_config:
      models:
        - model_name: gpt-3.5-turbo
          params:
            temperature: 0
        - model_name: gpt-3.5-turbo
          params:
            temperature: 1
  - name: prompt_template
    variations:
      - instantiated_value: |
          Answer following multiple choices question:
          Question: {question_text}
          A. {option_a}
          B. {option_b}
          C. {option_c}
          Answer:
        value: |
          Answer following multiple choices question:
          Question: {question_text}
          A. {option_a}
          B. {option_b}
          C. {option_c}
          Answer:
        value_type: str
        variation_id: instruct_question
      - instantiated_value: |
          Question: {question_text}
          A. {option_a}
          B. {option_b}
          C. {option_c}
          Answer:
        value: |
          Question: {question_text}
          A. {option_a}
          B. {option_b}
          C. {option_c}
          Answer:
        value_type: str
        variation_id: simple

human_rating_configs:
  - name: correctness
    instructions: Rate whether the answer clearly state what the correct answer is
    scale: [1, 5]

  - name: coherence
    instructions: Rate whether the answer and explanation are coherent
    scale: [1, 5]
