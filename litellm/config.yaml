model_list:
  # only enable those models don't supprort file and batch.
  # Alibaba
  - model_name: "alibaba/*"
    litellm_params:
      model: "openai/*"
      api_key: os.environ/DASHSCOPE_API_KEY
      api_base: "https://dashscope.aliyuncs.com/compatible-mode/v1"
 # LLAMA on fireworks
  - model_name: "fireworks_ai/*"
    litellm_params:
      model: "fireworks_ai/*"
      api_key: os.environ/FIREWORKS_API_KEY
 # XAI
  - model_name: "xai/*"
    litellm_params:
      model: "xai/*"
      api_key: os.environ/XAI_API_KEY


general_settings:
  # Enable detailed logging for debugging
  debug: true
  # Set a master key for proxy authentication
  master_key: "os.environ/LITELLM_MASTER_KEY"


litellm_settings:
  cache: true
  cache_params:
    type: redis
    ttl: 10368000  # 120 days
    host: "127.0.0.1"
    port: "6379"


files_settings:
  - custom_llm_provider: openai
    api_key: os.environ/OPENAI_API_KEY
    organization: os.environ/OPENAI_ORG_ID
  - custom_llm_provider: vertex_ai
    vertex_location: "us-central1"
    vertex_project: os.environ/VERTEXAI_PROJECT
    vertex_credentials: os.environ/VERTEX_SERVICE_ACCOUNT_CREDENTIALS

