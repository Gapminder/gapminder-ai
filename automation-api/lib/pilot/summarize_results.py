"""
Automate processing of experiment results from JSONL files to Parquet summaries.
"""
import re
import json
import argparse
import logging
import polars as pl
from pathlib import Path
from typing import List, Dict, Tuple, Optional

logger = logging.getLogger(__name__)

def get_evaluator_prefix(evaluator_id: str) -> str:
    """Map evaluator ID to column prefix using simple substring matching."""
    evaluator_id = evaluator_id.lower()
    if 'claude' in evaluator_id:
        return 'claude'
    elif 'gpt' in evaluator_id:
        return 'gpt'
    elif 'gemini' in evaluator_id:
        return 'gemini'
    return evaluator_id

def find_file_groups(folder: Path) -> Dict[str, Tuple[Path, List[Path]]]:
    """Group response files with their corresponding eval files."""
    file_pattern = re.compile(
        r"(?P<model_id>.+?)-question_prompts-response"
        r"(-eval-prompts-(?P<evaluator_id>.+?)-response)?\.jsonl$"
    )
    
    groups = {}
    eval_files = []
    
    for path in folder.glob("*.jsonl"):
        match = file_pattern.match(path.name)
        if not match:
            continue
            
        model_id = match.group("model_id")
        if not match.group("evaluator_id"):
            groups[model_id] = (path, [])
        else:
            eval_files.append((model_id, path))
    
    # Attach eval files to their response groups
    for model_id, eval_path in eval_files:
        if model_id in groups:
            groups[model_id][1].append(eval_path)
    
    return groups

def load_jsonl(file_path: Path) -> List[Dict]:
    """Load a JSONL file into a list of dictionaries."""
    with file_path.open(encoding="utf-8") as f:
        return [json.loads(line) for line in f]

def extract_custom_id_info(custom_id: str, expected_model_id: str) -> Dict[str, str]:
    """Parse custom_id structure and validate model ID."""
    parts = [p for p in custom_id.split("-") if p not in {"question", "q", "eval"}]
    
    if parts[0] != expected_model_id:
        raise ValueError(f"Model ID mismatch in custom_id: {custom_id}")
    
    field_names = ["model_config_id", "question_id", "prompt_variation_id"]
    if len(parts) > 3:
        field_names.append("metric_id")
    
    return dict(zip(field_names, parts))

def extract_score(eval_text: str) -> int:
    """Extract score from evaluation text (A=0, B=1, C=2, D=3)."""
    grade_map = {"A": 0, "B": 1, "C": 2, "D": 3}
    last_word = eval_text.strip().split()[-1].upper()
    return grade_map.get(last_word, -1)

def process_responses(responses: List[Dict], model_id: str) -> List[Dict]:
    """Process response records into structured data."""
    processed = []
    for resp in responses:
        info = extract_custom_id_info(resp["custom_id"], model_id)
        processed.append({
            "model_config_id": info["model_config_id"],
            "question_id": info["question_id"],
            "prompt_variation_id": info["prompt_variation_id"],
            "response": resp.get("content", "NOT_ANSWERED")
        })
    return processed

def process_evals(evals: List[Dict], evaluator_prefix: str, model_id: str) -> List[Dict]:
    """Process evaluation records into structured scores."""
    processed = []
    for eval_rec in evals:
        info = extract_custom_id_info(eval_rec["custom_id"], model_id)
        processed.append({
            "model_config_id": info["model_config_id"],
            "question_id": info["question_id"],
            "prompt_variation_id": info["prompt_variation_id"],
            "metric_id": info.get("metric_id", ""),
            f"{evaluator_prefix}_score": extract_score(eval_rec.get("content", ""))
        })
    return processed

def pivot_eval_df(df: pl.DataFrame, evaluator_prefix: str) -> pl.DataFrame:
    """Pivot evaluation dataframe to metric-based columns."""
    return df.pivot(
        values=f"{evaluator_prefix}_score",
        index=["model_config_id", "question_id", "prompt_variation_id"],
        on="metric_id",
        aggregate_function="first"
    ).rename(lambda c: f"{evaluator_prefix}_{c}" if c != "metric_id" else c)

def calculate_final_score(scores: List[int]) -> int:
    """Determine final score by majority vote."""
    score_counts: Dict[int, int] = {}
    for score in scores:
        score_counts[score] = score_counts.get(score, 0) + 1
    
    max_count = max(score_counts.values(), default=0)
    return next((s for s, c in score_counts.items() if c == max_count), 0)

def process_group(response_path: Path, eval_paths: List[Path], output_dir: Path) -> None:
    """Process a group of response + eval files into final output."""
    model_id = response_path.name.split("-")[0]
    
    # Load and process data
    responses = load_jsonl(response_path)
    response_df = pl.DataFrame(process_responses(responses, model_id))
    
    # Process evaluations
    eval_dfs = []
    for eval_path in eval_paths:
        evaluator_id = eval_path.stem.split("-")[-2]
        prefix = get_evaluator_prefix(evaluator_id)
        evals = load_jsonl(eval_path)
        eval_df = pl.DataFrame(process_evals(evals, prefix, model_id))
        eval_dfs.append(pivot_eval_df(eval_df, prefix))
    
    # Combine all data
    combined_df = response_df
    for df in eval_dfs:
        combined_df = combined_df.join(
            df, 
            on=["model_config_id", "question_id", "prompt_variation_id"],
            how="left"
        )
    
    # Add final correctness score
    score_cols = [c for c in combined_df.columns if c.endswith("_correctness")]
    combined_df = combined_df.with_columns(
        pl.struct(score_cols).map_elements(
            lambda x: calculate_final_score(x.values()),
            return_dtype=pl.Int32
        ).alias("final_correctness")
    )
    
    # Write output
    output_path = output_dir / f"{model_id}_output.parquet"
    combined_df.write_parquet(output_path)
    logger.info(f"Processed {model_id} â†’ {output_path}")

def main():
    """Command line interface for processing experiment results."""
    parser = argparse.ArgumentParser(
        description="Generate consolidated Parquet files from experiment JSONL results"
    )
    parser.add_argument(
        "input_dir", 
        type=Path,
        help="Directory containing response/eval JSONL files"
    )
    parser.add_argument(
        "-o", "--output-dir",
        type=Path,
        default=None,
        help="Output directory (default: same as input)"
    )
    
    args = parser.parse_args()
    output_dir = args.output_dir or args.input_dir
    output_dir.mkdir(exist_ok=True)
    
    file_groups = find_file_groups(args.input_dir)
    for model_id, (resp_path, eval_paths) in file_groups.items():
        logger.info(f"Processing {model_id}...")
        process_group(resp_path, eval_paths, output_dir)

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main()
